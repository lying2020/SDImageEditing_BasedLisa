#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é’ˆå¯¹ 009.png çš„ç¼–è¾‘ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•å°†é’è¾£æ¤’æ›¿æ¢ä¸ºèƒ¡èåœ
"""

import os
import sys
import torch
from PIL import Image
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„ï¼ˆåŒ…å«ä¿®æ”¹åçš„ diffusersï¼‰
src_path = os.path.join(project_root, "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# è®¾ç½®æ¨¡å‹è·¯å¾„
os.environ["SMARTFREEEDIT_MODEL_PATH"] = "/home/liying/Documents/smart_free_edit_huggingface"

from diffusers import StableDiffusionBrushNetPipeline, BrushNetModel, UniPCMultistepScheduler
from SmartFreeEdit.src.smartfreeedit_all_pipeline import SmartFreeEdit_Pipeline
from SmartFreeEdit.utils.utils import load_grounding_dino_model
from SmartFreeEdit.utils.utils_lisa import load_lisa_model
from SmartFreeEdit.src.vlm_pipeline import (
    vlm_response_editing_type,
    vlm_response_object_wait_for_edit,
    vlm_response_mask,
    vlm_response_prompt_after_apply_instruction
)

def load_models():
    """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
    print("=" * 60)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print("=" * 60)

    # è·å–æ¨¡å‹è·¯å¾„
    try:
        from SmartFreeEdit.config_local import (
            SMARTFREEEDIT_MODEL_PATH,
            DEFAULT_BASE_MODEL_PATH,
            BRUSHNET_PATH,
            LISA_PATH,
            GROUNDINGDINO_PATH
        )
        model_path = SMARTFREEEDIT_MODEL_PATH
        base_model_path = DEFAULT_BASE_MODEL_PATH
        brushnet_path = BRUSHNET_PATH
        lisa_path = LISA_PATH
        groundingdino_path = GROUNDINGDINO_PATH
    except ImportError:
        model_path = os.getenv("SMARTFREEEDIT_MODEL_PATH", "/home/liying/Documents/smart_free_edit_huggingface")
        base_model_path = os.path.join(model_path, "base_model/realisticVisionV60B1_v51VAE")
        brushnet_path = os.path.join(model_path, "checkpoint-100000/brushnet")
        lisa_path = os.path.join(model_path, "LISA-7B-v1-explanatory")
        groundingdino_path = os.path.join(model_path, "grounding_dino/groundingdino_swint_ogc.pth")

    torch_dtype = torch.float16
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½BrushNetå’ŒåŸºç¡€æ¨¡å‹
    print("\n[1/4] åŠ è½½BrushNetå’ŒåŸºç¡€æ¨¡å‹...")
    brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=torch_dtype)
    pipe = StableDiffusionBrushNetPipeline.from_pretrained(
        base_model_path, brushnet=brushnet, torch_dtype=torch_dtype, low_cpu_mem_usage=False
    )
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.enable_model_cpu_offload()
    print("âœ… BrushNetå’ŒåŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

    # 2. åŠ è½½GroundingDINO
    print("\n[2/4] åŠ è½½GroundingDINO...")
    config_file = os.path.join(project_root, "SmartFreeEdit/utils/GroundingDINO_SwinT_OGC.py")
    groundingdino_model = load_grounding_dino_model(config_file, groundingdino_path, device=device)
    print("âœ… GroundingDINOåŠ è½½å®Œæˆ")

    # 3. åŠ è½½LISA
    print("\n[3/4] åŠ è½½LISAæ¨¡å‹...")
    lisa_model, tokenizer = load_lisa_model(
        version=lisa_path,
        precision="fp16",
        load_in_8bit=True,
        load_in_4bit=False,
        vision_tower="openai/clip-vit-large-patch14",
        local_rank=0
    )
    print("âœ… LISAæ¨¡å‹åŠ è½½å®Œæˆ")

    print("\n" + "=" * 60)
    print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 60)

    return pipe, groundingdino_model, lisa_model, tokenizer, device


def edit_009_image(pipe, groundingdino_model, lisa_model, tokenizer, device,
                   input_image_path, prompt, output_path,
                   api_key, api_version, end_point, engine):
    """
    ç¼–è¾‘ 009.png å›¾ç‰‡ï¼šå°†é’è¾£æ¤’æ›¿æ¢ä¸ºèƒ¡èåœ

    å‚æ•°:
        prompt: ç¼–è¾‘æŒ‡ä»¤ï¼Œä¾‹å¦‚ "replace the green pepper with a carrot"
    """
    print("\n" + "=" * 60)
    print("å¼€å§‹ç¼–è¾‘å›¾ç‰‡...")
    print("=" * 60)
    print(f"è¾“å…¥å›¾ç‰‡: {input_image_path}")
    print(f"ç¼–è¾‘æŒ‡ä»¤: {prompt}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")

    # åŠ è½½å›¾ç‰‡
    original_image = np.array(Image.open(input_image_path).convert("RGB"))
    print(f"å›¾ç‰‡å°ºå¯¸: {original_image.shape}")

    # æ„å»ºAPI URL
    url = f"{end_point}/openai/deployments/{engine}/chat/completions?api-version={api_version}"

    # 1. ç¡®å®šç¼–è¾‘ç±»åˆ«
    print("\n[æ­¥éª¤1/5] ç¡®å®šç¼–è¾‘ç±»åˆ«...")
    category = vlm_response_editing_type(url, api_key, original_image, prompt, device)
    print(f"âœ… ç¼–è¾‘ç±»åˆ«: {category}")

    # 2. ç¡®å®šç¼–è¾‘å¯¹è±¡
    print("\n[æ­¥éª¤2/5] ç¡®å®šç¼–è¾‘å¯¹è±¡...")
    object_wait_for_edit = vlm_response_object_wait_for_edit(
        url, api_key, original_image, category, prompt, device
    )
    print(f"âœ… ç¼–è¾‘å¯¹è±¡: {object_wait_for_edit}")

    # 3. ç”Ÿæˆmask
    print("\n[æ­¥éª¤3/5] ç”Ÿæˆmask...")
    original_mask = vlm_response_mask(
        url, api_key, category, original_image, prompt,
        object_wait_for_edit, lisa_model, tokenizer, device
    ).astype(np.uint8)
    print("âœ… Maskç”Ÿæˆå®Œæˆ")

    # ä¿å­˜maskï¼ˆå¯é€‰ï¼‰
    mask_save_path = output_path.replace(".png", "_mask.png")
    Image.fromarray(original_mask.squeeze()).save(mask_save_path)
    print(f"âœ… Maskå·²ä¿å­˜åˆ°: {mask_save_path}")

    # 4. ç”Ÿæˆç›®æ ‡æç¤ºè¯
    print("\n[æ­¥éª¤4/5] ç”Ÿæˆç›®æ ‡æç¤ºè¯...")
    target_prompt = vlm_response_prompt_after_apply_instruction(
        url, api_key, original_image, prompt, category, device
    )
    print(f"âœ… ç›®æ ‡æç¤ºè¯: {target_prompt}")

    # 5. æ‰§è¡Œç¼–è¾‘
    print("\n[æ­¥éª¤5/5] æ‰§è¡Œå›¾åƒç¼–è¾‘...")
    generator = torch.Generator(device).manual_seed(42)
    with torch.autocast(device):
        images, mask_image, mask_np, init_image_np = SmartFreeEdit_Pipeline(
            pipe,
            target_prompt,
            original_mask,
            original_image,
            generator,
            num_inference_steps=50,
            guidance_scale=7.5,
            control_strength=1.0,
            negative_prompt="ugly, low quality, distorted",
            num_samples=1,
            blending=True
        )

    # ä¿å­˜ç»“æœ
    images[0].save(output_path)
    print(f"\nâœ… ç¼–è¾‘å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("=" * 60)

    return images[0], original_mask, target_prompt


if __name__ == "__main__":
    print("=" * 60)
    print("009.png ç¼–è¾‘ç¤ºä¾‹ï¼šå°†é’è¾£æ¤’æ›¿æ¢ä¸ºèƒ¡èåœ")
    print("=" * 60)

    # ============================================
    # é…ç½®å‚æ•°
    # ============================================
    # å›¾ç‰‡è·¯å¾„
    input_image_path = "/home/liying/Desktop/IMAGE_EDITE-CVPR-2025/images/ReasonEdit/6-Reasoning/009.png"

    # ç¼–è¾‘æŒ‡ä»¤ï¼ˆä½ çš„éœ€æ±‚ï¼šæŠŠé’è¾£æ¤’æ¢æˆèƒ¡èåœï¼‰
    # æ–¹å¼1ï¼šç›´æ¥æ›¿æ¢ï¼ˆæ¨èï¼‰
    prompt = "replace the green pepper with a carrot"

    # æ–¹å¼2ï¼šæ¨ç†å¼æ›¿æ¢ï¼ˆç±»ä¼¼å®˜æ–¹é£æ ¼ï¼‰
    # prompt = "What is the green spicy vegetable? Please replace it with a carrot."

    # æ–¹å¼3ï¼šè¯¦ç»†æè¿°
    # prompt = "replace the green pepper (the spicy vegetable) with a fresh orange carrot"

    # è¾“å‡ºè·¯å¾„
    output_path = "./output_009_carrot.png"

    # GPT-4o API é…ç½®ï¼ˆéœ€è¦é…ç½®ï¼‰
    api_key = "your_api_key"  # âš ï¸ è¯·æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    api_version = "2024-08-01-preview"
    end_point = "https://your-endpoint.openai.azure.com/"  # âš ï¸ è¯·æ›¿æ¢ä¸ºä½ çš„ç«¯ç‚¹
    engine = "4o"

    # ============================================
    # æ£€æŸ¥é…ç½®
    # ============================================
    if api_key == "your_api_key" or end_point == "https://your-endpoint.openai.azure.com/":
        print("\nâš ï¸  è­¦å‘Šï¼šè¯·å…ˆé…ç½® GPT-4o API å‚æ•°ï¼")
        print("   ä¿®æ”¹è„šæœ¬ä¸­çš„ api_key å’Œ end_point å˜é‡")
        print("\næˆ–è€…ä½¿ç”¨ç®€å•æ¨ç†æ¨¡å¼ï¼ˆéœ€è¦æ‰‹åŠ¨æä¾›maskï¼‰")
        sys.exit(1)

    if not os.path.exists(input_image_path):
        print(f"\nâŒ é”™è¯¯ï¼šè¾“å…¥å›¾ç‰‡ä¸å­˜åœ¨: {input_image_path}")
        sys.exit(1)

    # ============================================
    # åŠ è½½æ¨¡å‹
    # ============================================
    pipe, groundingdino_model, lisa_model, tokenizer, device = load_models()

    # ============================================
    # æ‰§è¡Œç¼–è¾‘
    # ============================================
    try:
        result_image, mask, target_prompt = edit_009_image(
            pipe=pipe,
            groundingdino_model=groundingdino_model,
            lisa_model=lisa_model,
            tokenizer=tokenizer,
            device=device,
            input_image_path=input_image_path,
            prompt=prompt,
            output_path=output_path,
            api_key=api_key,
            api_version=api_version,
            end_point=end_point,
            engine=engine
        )

        print("\nğŸ‰ ç¼–è¾‘æˆåŠŸå®Œæˆï¼")
        print(f"   ç»“æœå›¾ç‰‡: {output_path}")
        print(f"   Maskå›¾ç‰‡: {output_path.replace('.png', '_mask.png')}")
        print(f"   ç›®æ ‡æç¤ºè¯: {target_prompt}")

    except Exception as e:
        print(f"\nâŒ ç¼–è¾‘å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
