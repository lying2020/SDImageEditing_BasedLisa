# SmartFreeEdit 项目分析报告

## 1. 项目概述与目录结构

### 1.1 项目用途

**SmartFreeEdit** 是一个基于 **Mask-Free Spatial-Aware** 的图像编辑系统，主要特点：

- **无需手动提供 mask**：通过自然语言指令自动理解编辑需求并生成 mask
- **复杂指令理解**：使用 MLLM（多模态大语言模型）理解复杂的编辑指令
- **支持多种编辑操作**：
  - 添加对象（Addition）
  - 删除对象（Remove）
  - 局部编辑（Local）
  - 全局编辑（Global）
  - 背景替换（Background）
  - 调整大小（Resize）

### 1.2 核心功能架构

项目采用**三阶段流水线**：

1. **MLLM驱动的Promptist**：将指令分解为编辑对象、类别和目标提示词
2. **推理分割（Reasoning Segmentation）**：将提示词转换为推理查询并生成推理 mask
3. **基于Inpainting的图像编辑器**：使用超图计算模块增强全局图像结构理解

### 1.3 目录结构

```
SDImageEditing_SmartFreeEdit/
├── SmartFreeEdit/              # 核心代码目录
│   ├── src/                   # 主要源代码
│   │   ├── smartfreeedit_app.py          # Gradio Web界面（主入口）
│   │   ├── smartfreeedit_all_pipeline.py # 核心编辑流水线
│   │   ├── vlm_pipeline.py               # VLM（视觉语言模型）处理
│   │   ├── vlm_template.py               # VLM模型配置模板
│   │   ├── base_model_template.py        # 基础模型配置模板
│   │   └── aspect_ratio_template.py     # 宽高比配置
│   ├── model/                 # 模型实现
│   │   ├── LISA.py            # LISA模型（推理分割）
│   │   ├── llava/             # LLaVA模型相关
│   │   └── segment_anything/  # SAM（Segment Anything Model）
│   ├── utils/                 # 工具函数
│   │   ├── utils.py           # GroundingDINO工具
│   │   ├── utils_lisa.py      # LISA工具
│   │   └── config.py          # 配置管理
│   └── download.py            # 模型下载脚本
├── src/diffusers/             # 修改的diffusers库（包含BrushNet）
├── test/                      # 测试脚本
│   └── ReasonEdit_test.py     # ReasonEdit基准测试
├── train/                     # 训练相关
│   └── brushnet/              # BrushNet训练代码
├── requirements.txt           # Python依赖
└── README.md                  # 项目说明
```

### 1.4 核心组件说明

| 组件 | 作用 | 位置 |
|------|------|------|
| **BrushNet** | 图像编辑的核心控制网络 | `src/diffusers/pipelines/` |
| **LISA-7B** | 推理分割模型，生成mask | `SmartFreeEdit/model/LISA.py` |
| **GroundingDINO** | 目标检测，定位编辑对象 | `SmartFreeEdit/utils/utils.py` |
| **VLM (GPT-4o)** | 理解编辑指令，生成目标提示词 | `SmartFreeEdit/src/vlm_pipeline.py` |
| **Base Model** | Stable Diffusion基础模型 | 用户模型目录 |

---

## 2. 使用本地模型进行Inference

### 2.1 模型路径配置

**当前代码中的模型路径硬编码为 `/models`**，需要修改为你的本地路径：

**你的模型路径**：`/home/liying/Documents/smart_free_edit_huggingface/`

### 2.2 修改配置文件

需要修改以下文件以使用本地模型：

#### 2.2.1 修改 `SmartFreeEdit/src/smartfreeedit_app.py`

```python
# 第54行附近，修改模型路径
SmartFreeEdit_path = "/home/liying/Documents/smart_free_edit_huggingface"  # 改为你的路径

# 第65-67行，确认路径正确
base_model_path = os.path.join(SmartFreeEdit_path, "base_model/realisticVisionV60B1_v51VAE")
brushnet_path = os.path.join(SmartFreeEdit_path, "checkpoint-100000/brushnet")
lisa_path = os.path.join(SmartFreeEdit_path, "LISA-7B-v1-explanatory")
```

#### 2.2.2 修改 `SmartFreeEdit/src/base_model_template.py`

```python
# 第12行，修改基础路径
SmartFreeEdit_path = "/home/liying/Documents/smart_free_edit_huggingface"  # 改为你的路径

# 第13行
brushnet_path = os.path.join(SmartFreeEdit_path, "checkpoint-100000/brushnet")

# 第20-37行，修改所有base_model路径
base_models_list = [
    {
        "name": "realisticVision (Default)",
        "local_path": os.path.join(SmartFreeEdit_path, "base_model/realisticVisionV60B1_v51VAE"),
        "pipe": ""  # 留空，运行时加载
    },
    # ... 其他模型类似
]
```

### 2.3 运行Inference的三种方式

#### 方式1：Gradio Web界面（推荐，最简单）

```bash
cd /home/liying/Desktop/IMAGE_EDITE-CVPR-2025/SDImageEditing_SmartFreeEdit

# 设置环境变量
export PYTHONPATH=.:$PYTHONPATH
export CUDA_VISIBLE_DEVICES=0

# 运行Gradio应用
python SmartFreeEdit/src/smartfreeedit_app.py
```

**访问**：浏览器打开 `http://localhost:8080`

**功能**：
- 上传图片
- 输入编辑指令（如 "remove the car"）
- 自动生成mask并完成编辑
- 可调整参数（guidance_scale, num_inference_steps等）

#### 方式2：Python脚本推理（适合批量处理）

创建脚本 `inference_example.py`：

```python
import os
import torch
from PIL import Image
import numpy as np
from diffusers import StableDiffusionBrushNetPipeline, BrushNetModel, UniPCMultistepScheduler
from SmartFreeEdit.src.smartfreeedit_all_pipeline import SmartFreeEdit_Pipeline
from SmartFreeEdit.utils.utils import load_grounding_dino_model
from SmartFreeEdit.utils.utils_lisa import load_lisa_model
from SmartFreeEdit.src.vlm_pipeline import (
    vlm_response_editing_type,
    vlm_response_object_wait_for_edit,
    vlm_response_mask,
    vlm_response_prompt_after_apply_instruction
)

# 配置路径
SmartFreeEdit_path = "/home/liying/Documents/smart_free_edit_huggingface"
base_model_path = os.path.join(SmartFreeEdit_path, "base_model/realisticVisionV60B1_v51VAE")
brushnet_path = os.path.join(SmartFreeEdit_path, "checkpoint-100000/brushnet")
lisa_path = os.path.join(SmartFreeEdit_path, "LISA-7B-v1-explanatory")
groundingdino_path = os.path.join(SmartFreeEdit_path, "grounding_dino/groundingdino_swint_ogc.pth")

# 加载模型
print("Loading models...")
torch_dtype = torch.float16
device = "cuda"

# 加载BrushNet和基础模型
brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=torch_dtype)
pipe = StableDiffusionBrushNetPipeline.from_pretrained(
    base_model_path, brushnet=brushnet, torch_dtype=torch_dtype, low_cpu_mem_usage=False
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# 加载GroundingDINO
config_file = 'SmartFreeEdit/utils/GroundingDINO_SwinT_OGC.py'
groundingdino_model = load_grounding_dino_model(config_file, groundingdino_path, device=device)

# 加载LISA
lisa_model, tokenizer = load_lisa_model(
    version=lisa_path,
    precision="fp16",
    load_in_8bit=True,
    load_in_4bit=False,
    vision_tower="openai/clip-vit-large-patch14",
    local_rank=0
)

# 准备输入
input_image_path = "your_image.jpg"  # 替换为你的图片路径
original_image = np.array(Image.open(input_image_path).convert("RGB"))
prompt = "remove the car"  # 编辑指令

# GPT-4o API配置（需要配置）
API_KEY = "your_api_key"
API_VERSION = "2024-08-01-preview"
END_POINT = "https://your-endpoint.openai.azure.com/"
ENGINE = "4o"
url = f"{END_POINT}/openai/deployments/{ENGINE}/chat/completions?api-version={API_VERSION}"

# 推理流程
print("Processing...")

# 1. 确定编辑类别
category = vlm_response_editing_type(url, API_KEY, original_image, prompt, device)
print(f"Category: {category}")

# 2. 确定编辑对象
object_wait_for_edit = vlm_response_object_wait_for_edit(
    url, API_KEY, original_image, category, prompt, device
)
print(f"Object to edit: {object_wait_for_edit}")

# 3. 生成mask
original_mask = vlm_response_mask(
    url, API_KEY, category, original_image, prompt,
    object_wait_for_edit, lisa_model, tokenizer, device
).astype(np.uint8)

# 4. 生成目标提示词
target_prompt = vlm_response_prompt_after_apply_instruction(
    url, API_KEY, original_image, prompt, category, device
)
print(f"Target prompt: {target_prompt}")

# 5. 执行编辑
generator = torch.Generator(device).manual_seed(42)
with torch.autocast(device):
    images, mask_image, mask_np, init_image_np = SmartFreeEdit_Pipeline(
        pipe,
        target_prompt,
        original_mask,
        original_image,
        generator,
        num_inference_steps=50,
        guidance_scale=7.5,
        control_strength=1.0,
        negative_prompt="ugly, low quality",
        num_samples=1,
        blending=True
    )

# 保存结果
images[0].save("output.png")
print("Done! Saved to output.png")
```

#### 方式3：使用测试脚本（ReasonEdit基准测试）

```bash
python test/ReasonEdit_test.py \
    --save_dir ./edited_images \
    --ReasonEdit_benchmark_dir /path/to/ReasonEdit/benchmark \
    --api_key your_api_key \
    --api_version 2024-08-01-preview \
    --end_point https://your-endpoint.openai.azure.com/ \
    --engine 4o
```

### 2.4 示例：简单推理脚本

创建一个简化的推理示例 `simple_inference.py`：

```python
"""
简单的推理示例 - 需要手动提供mask
"""
import torch
from PIL import Image
from diffusers import StableDiffusionBrushNetPipeline, BrushNetModel, UniPCMultistepScheduler

# 模型路径
base_model_path = "/home/liying/Documents/smart_free_edit_huggingface/base_model/realisticVisionV60B1_v51VAE"
brushnet_path = "/home/liying/Documents/smart_free_edit_huggingface/checkpoint-100000/brushnet"

# 加载模型
brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=torch.float16)
pipe = StableDiffusionBrushNetPipeline.from_pretrained(
    base_model_path, brushnet=brushnet, torch_dtype=torch.float16
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# 加载图片和mask
init_image = Image.open("input.jpg").convert("RGB")
mask_image = Image.open("mask.jpg").convert("RGB")  # 白色区域为编辑区域

# 推理
prompt = "a beautiful landscape"
images = pipe(
    prompt,
    init_image,
    mask_image,
    num_inference_steps=50,
    guidance_scale=7.5,
    brushnet_conditioning_scale=1.0
).images

# 保存
images[0].save("output.jpg")
print("Done!")
```

---

## 3. 模型需求与算力分析

### 3.1 所需模型清单

#### 3.1.1 已拥有的模型（检查结果）

根据你的模型目录 `/home/liying/Documents/smart_free_edit_huggingface/`：

| 模型 | 路径 | 状态 | 大小估算 |
|------|------|------|---------|
| **Base Models** | `base_model/` | ✅ 已拥有 | ~20-30 GB |
| - realisticVisionV60B1_v51VAE | `base_model/realisticVisionV60B1_v51VAE/` | ✅ | ~4-6 GB |
| - henmixReal_v5c | `base_model/henmixReal_v5c/` | ✅ | ~4-6 GB |
| - meinamix_meinaV11 | `base_model/meinamix_meinaV11/` | ✅ | ~4-6 GB |
| - dreamshaper_8 | `base_model/dreamshaper_8/` | ✅ | ~4-6 GB |
| - epicrealism_naturalSinRC1VAE | `base_model/epicrealism_naturalSinRC1VAE/` | ✅ | ~4-6 GB |
| **BrushNet** | `checkpoint-100000/brushnet/` | ✅ 已拥有 | ~5 GB |
| **LISA-7B** | `LISA-7B-v1-explanatory/` | ✅ 已拥有 | ~14 GB |
| **GroundingDINO** | `grounding_dino/groundingdino_swint_ogc.pth` | ✅ 已拥有 | ~500 MB |

#### 3.1.2 缺少的模型/依赖

| 组件 | 状态 | 说明 |
|------|------|------|
| **GPT-4o API** | ⚠️ 需要配置 | 需要Azure OpenAI API密钥（或使用其他VLM） |
| **CLIP Vision Tower** | ⚠️ 自动下载 | `openai/clip-vit-large-patch14`（首次运行自动下载） |
| **Segment Anything** | ⚠️ 可能需要 | 如果使用SAM功能（LISA已包含） |

### 3.2 模型大小详细分析

```
总模型大小估算：~50-60 GB

详细分解：
├── Base Models (5个)          ~25 GB
│   ├── UNet (每个 ~3-5 GB)    ~20 GB
│   ├── VAE (每个 ~300 MB)     ~1.5 GB
│   ├── Text Encoder (每个 ~200 MB) ~1 GB
│   └── Safety Checker (每个 ~500 MB) ~2.5 GB
├── BrushNet                   ~5 GB
├── LISA-7B                    ~14 GB
├── GroundingDINO              ~500 MB
└── 其他依赖（CLIP等）          ~5 GB
```

### 3.3 算力需求分析

#### 3.3.1 推理（Inference）算力需求

| 配置等级 | GPU显存 | CPU内存 | 适用场景 | 推理速度 |
|---------|---------|---------|---------|---------|
| **最低配置** | 16 GB | 32 GB | 单张图片，512x512，启用CPU offload | 慢（~30-60秒/张） |
| **推荐配置** | 24 GB | 40 GB | 标准推理，512x512，少量CPU offload | 中等（~20-40秒/张） |
| **理想配置** | 40 GB+ | 64 GB+ | 批量处理，高分辨率（768x768+） | 快（~10-20秒/张） |

**显存占用分析**（推理时）：
```
模型加载显存占用：
├── Base Model (UNet)          ~4-5 GB
├── BrushNet                   ~1-2 GB
├── LISA-7B (8-bit量化)        ~8-10 GB
├── GroundingDINO               ~500 MB
├── VAE                         ~500 MB
└── Text Encoder                ~200 MB
─────────────────────────────────────
总计（峰值）                    ~15-18 GB
```

**优化建议**：
1. **启用CPU offload**：`pipe.enable_model_cpu_offload()`（已默认）
2. **LISA使用8-bit量化**：`load_in_8bit=True`（已默认）
3. **降低分辨率**：512x512 → 384x384（节省~30%显存）
4. **使用xFormers**：加速注意力计算，节省显存

#### 3.3.2 训练（Training）算力需求

| 配置等级 | GPU数量 | 单卡显存 | Batch Size | 训练速度 | 适用场景 |
|---------|---------|---------|------------|---------|---------|
| **最低配置** | 1 | 16 GB | 4 | 慢 | 小规模实验 |
| **推荐配置** | 2-4 | 24 GB | 8 | 中等 | 标准训练 |
| **理想配置** | 4-8 | 40 GB+ | 16+ | 快 | 大规模训练 |

**训练显存占用**（单卡，batch_size=8）：
```
├── Base Model (UNet)          ~4-5 GB
├── BrushNet                   ~2-3 GB
├── 梯度（Gradients）           ~4-5 GB
├── 优化器状态（Adam）          ~8-10 GB
└── 激活值（Activations）       ~2-3 GB
─────────────────────────────────────
总计（峰值）                    ~20-26 GB
```

**训练优化建议**：
1. **梯度累积**：`--gradient_accumulation_steps 2`（batch_size=4时等效batch_size=8）
2. **混合精度训练**：`--mixed_precision fp16`
3. **梯度检查点**：`--gradient_checkpointing`（节省~30%显存）
4. **多卡训练**：使用`accelerate launch`进行分布式训练

### 3.4 实际运行建议

#### 对于你的环境（假设）

**如果GPU显存 < 24GB**：
```python
# 在 smartfreeedit_app.py 中
pipe.enable_model_cpu_offload()  # 已启用
# LISA使用8-bit量化（已启用）
# 降低默认分辨率到384x384
```

**如果GPU显存 >= 24GB**：
```python
# 可以尝试更高分辨率
# 可以增加num_samples（生成多张结果）
# 可以禁用部分CPU offload以加速
```

**如果GPU显存 >= 40GB**：
```python
# 可以加载多个base_model到显存
# 可以批量处理多张图片
# 可以使用更高分辨率（768x768+）
```

### 3.5 性能基准测试

**参考性能**（基于24GB GPU，512x512分辨率）：

| 操作 | 时间 | 说明 |
|------|------|------|
| 模型加载 | ~30-60秒 | 首次加载 |
| VLM推理（GPT-4o） | ~2-5秒 | 网络延迟+API处理 |
| Mask生成（LISA） | ~3-5秒 | 推理分割 |
| 图像编辑（BrushNet） | ~15-30秒 | 50步推理 |
| **总计** | **~20-40秒/张** | 单张图片完整流程 |

---

## 4. 快速开始指南

### 4.1 环境配置

```bash
# 1. 创建conda环境
conda create -n smartfreeedit python=3.10.6 -y
conda activate smartfreeedit

# 2. 安装PyTorch（CUDA 11.7）
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 \
    --index-url https://download.pytorch.org/whl/cu117

# 3. 安装项目依赖
cd /home/liying/Desktop/IMAGE_EDITE-CVPR-2025/SDImageEditing_SmartFreeEdit
pip install -e .
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

### 4.2 修改模型路径

修改以下文件中的路径为你的本地路径：
- `SmartFreeEdit/src/smartfreeedit_app.py` (第54行)
- `SmartFreeEdit/src/base_model_template.py` (第12行)

### 4.3 配置GPT-4o API（可选，但推荐）

在Gradio界面中配置：
- API Key
- API Version: `2024-08-01-preview`
- End Point: 你的Azure OpenAI端点
- Engine: `4o`

### 4.4 运行

```bash
export PYTHONPATH=.:$PYTHONPATH
export CUDA_VISIBLE_DEVICES=0
python SmartFreeEdit/src/smartfreeedit_app.py
```

---

## 5. 总结

### 5.1 项目核心

- **用途**：无需mask的图像编辑系统，通过自然语言指令自动编辑图像
- **核心**：三阶段流水线（指令理解 → 推理分割 → 图像编辑）
- **优势**：支持复杂指令，自动生成mask，无需手动标注

### 5.2 模型状态

✅ **已拥有**：
- 5个Base Models（~25 GB）
- BrushNet（~5 GB）
- LISA-7B（~14 GB）
- GroundingDINO（~500 MB）

⚠️ **需要配置**：
- GPT-4o API密钥（或使用其他VLM）

### 5.3 算力需求

- **推理**：推荐24GB GPU，40GB CPU内存
- **训练**：推荐2-4张24GB GPU
- **优化**：已启用CPU offload和8-bit量化

### 5.4 下一步

1. ✅ 修改模型路径配置
2. ✅ 配置GPT-4o API（或使用其他VLM）
3. ✅ 运行Gradio界面测试
4. ✅ 根据需要调整显存优化参数

---

**报告生成时间**：2025年
**项目路径**：`/home/liying/Desktop/IMAGE_EDITE-CVPR-2025/SDImageEditing_SmartFreeEdit`
**模型路径**：`/home/liying/Documents/smart_free_edit_huggingface/`
